{"nbformat_minor": 1, "cells": [{"source": "## Tuning Model Parameters\n\nIn this exercise, you will optimise the parameters for a classification model.\n\n### Prepare the Data\n\nFirst, import the libraries you will need and prepare the training and test data:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "# Import Spark SQL and Spark ML libraries\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Load the source data\ncsv = spark.read.csv('wasb:///data/flights.csv', inferSchema=True, header=True)\n\n# Select features and label\ndata = csv.select(\"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \"DepDelay\", ((col(\"ArrDelay\") > 15).cast(\"Int\").alias(\"label\")))\n\n# Split the data\nsplits = data.randomSplit([0.7, 0.3])\ntrain = splits[0]\ntest = splits[1].withColumnRenamed(\"label\", \"trueLabel\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7</td><td>application_1577379476625_0011</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-myfirs.q5ahg2egvptepbjcwa345wefra.gx.internal.cloudapp.net:8088/proxy/application_1577379476625_0011/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-myfirs.q5ahg2egvptepbjcwa345wefra.gx.internal.cloudapp.net:30060/node/containerlogs/container_1577379476625_0011_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"scrolled": false, "cell_status": {"execute_time": {"duration": 11453.713134765625, "end_time": 1577468915031.956}}, "collapsed": false}}, {"source": "### Define the Pipeline\nNow define a pipeline that creates a feature vector and trains a classification model", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "# Define the pipeline\nassembler = VectorAssembler(inputCols = [\"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \"DepDelay\"], outputCol=\"features\")\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\npipeline = Pipeline(stages=[assembler, lr])", "outputs": [], "metadata": {"scrolled": false, "cell_status": {"execute_time": {"duration": 246.52978515625, "end_time": 1577468915288.255}}, "collapsed": false}}, {"source": "### Tune Parameters\nYou can tune parameters to find the best model for your data. A simple way to do this is to use  **TrainValidationSplit** to evaluate each combination of parameters defined in a **ParameterGrid** against a subset of the training data in order to find the best performing parameters.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.3, 0.1, 0.01]).addGrid(lr.maxIter, [10, 5]).addGrid(lr.threshold, [0.35, 0.30]).build()\ntvs = TrainValidationSplit(estimator=pipeline, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, trainRatio=0.8)\n\nmodel = tvs.fit(train)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 66553.46484375, "end_time": 1577469121389}}, "collapsed": false}}, {"source": "### Test the Model\nNow you're ready to apply the model to the test data.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "prediction = model.transform(test)\npredicted = prediction.select(\"features\", \"prediction\", \"probability\", \"trueLabel\")\npredicted.show(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+----------+--------------------+---------+\n|            features|prediction|         probability|trueLabel|\n+--------------------+----------+--------------------+---------+\n|[1.0,1.0,10140.0,...|       0.0|[0.90569012614223...|        0|\n|[1.0,1.0,10140.0,...|       0.0|[0.94817037969475...|        0|\n|[1.0,1.0,10140.0,...|       0.0|[0.93090326563520...|        0|\n|[1.0,1.0,10140.0,...|       0.0|[0.89489637276683...|        0|\n|[1.0,1.0,10140.0,...|       1.0|[0.01085626962635...|        1|\n|[1.0,1.0,10140.0,...|       1.0|[1.29099284389552...|        1|\n|[1.0,1.0,10140.0,...|       0.0|[0.92059334753097...|        0|\n|[1.0,1.0,10299.0,...|       0.0|[0.90506132939867...|        0|\n|[1.0,1.0,10299.0,...|       0.0|[0.93612003840582...|        0|\n|[1.0,1.0,10299.0,...|       1.0|[0.54321089665309...|        1|\n+--------------------+----------+--------------------+---------+\nonly showing top 10 rows"}], "metadata": {"scrolled": false, "cell_status": {"execute_time": {"duration": 2262.452880859375, "end_time": 1577469172232.166}}, "collapsed": false}}, {"source": "### Compute Confusion Matrix Metrics\nClassifiers are typically evaluated by creating a *confusion matrix*, which indicates the number of:\n- True Positives\n- True Negatives\n- False Positives\n- False Negatives\n\nFrom these core measures, other evaluation metrics such as *precision* and *recall* can be calculated.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "tp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 1\").count())\nfp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 0\").count())\ntn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 0\").count())\nfn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 1\").count())\nmetrics = spark.createDataFrame([\n (\"TP\", tp),\n (\"FP\", fp),\n (\"TN\", tn),\n (\"FN\", fn),\n (\"Precision\", tp / (tp + fp)),\n (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\nmetrics.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+------------------+\n|   metric|             value|\n+---------+------------------+\n|       TP|          119677.0|\n|       FP|           18544.0|\n|       TN|          630275.0|\n|       FN|           41967.0|\n|Precision|0.8658380419762555|\n|   Recall|0.7403739080943308|\n+---------+------------------+"}], "metadata": {"cell_status": {"execute_time": {"duration": 9306.932861328125, "end_time": 1577469189716.022}}, "collapsed": false}}, {"source": "### Review the Area Under ROC\nAnother way to assess the performance of a classification model is to measure the area under a ROC curve for the model. the spark.ml library includes a **BinaryClassificationEvaluator** class that you can use to compute this.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "evaluator = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naur = evaluator.evaluate(prediction)\nprint \"AUR = \", aur", "outputs": [{"output_type": "stream", "name": "stdout", "text": "AUR =  0.855896373777"}], "metadata": {"cell_status": {"execute_time": {"duration": 2269.570068359375, "end_time": 1577469209964.45}}, "collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}