{"nbformat_minor": 1, "cells": [{"source": "## Evaluating a Classification Model\n\nIn this exercise, you will create a pipeline for a classification model, and then apply commonly used metrics to evaluate the resulting classifier.\n\n### Prepare the Data\n\nFirst, import the libraries you will need and prepare the training and test data:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "# Import Spark SQL and Spark ML libraries\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Load the source data\ncsv = spark.read.csv('wasb:///data/flights.csv', inferSchema=True, header=True)\n\n# Select features and label\ndata = csv.select(\"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \"DepDelay\", ((col(\"ArrDelay\") > 15).cast(\"Int\").alias(\"label\")))\n\n# Split the data\nsplits = data.randomSplit([0.7, 0.3])\ntrain = splits[0]\ntest = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1577379476625_0009</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-myfirs.q5ahg2egvptepbjcwa345wefra.gx.internal.cloudapp.net:8088/proxy/application_1577379476625_0009/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn1-myfirs.q5ahg2egvptepbjcwa345wefra.gx.internal.cloudapp.net:30060/node/containerlogs/container_1577379476625_0009_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 11384.031982421875, "end_time": 1577467563263.645}}, "collapsed": false}}, {"source": "### Define the Pipeline and Train the Model\nNow define a pipeline that creates a feature vector and trains a classification model", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "# Define the pipeline\nassembler = VectorAssembler(inputCols = [\"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \"DepDelay\"], outputCol=\"features\")\nlr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3)\npipeline = Pipeline(stages=[assembler, lr])\n# Train the model\nmodel = pipeline.fit(train)", "outputs": [], "metadata": {"scrolled": false, "cell_status": {"execute_time": {"duration": 9337.546142578125, "end_time": 1577467604684.609}}, "collapsed": false}}, {"source": "### Test the Model\nNow you're ready to apply the model to the test data.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "prediction = model.transform(test)\npredicted = prediction.select(\"features\", \"prediction\", \"trueLabel\")\npredicted.show(10, truncate=False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------------------------------+----------+---------+\n|features                       |prediction|trueLabel|\n+-------------------------------+----------+---------+\n|[1.0,1.0,10140.0,10397.0,-4.0] |0.0       |0        |\n|[1.0,1.0,10140.0,11292.0,2.0]  |0.0       |0        |\n|[1.0,1.0,10140.0,11298.0,-6.0] |0.0       |0        |\n|[1.0,1.0,10140.0,12266.0,27.0] |0.0       |1        |\n|[1.0,1.0,10140.0,12266.0,838.0]|1.0       |1        |\n|[1.0,1.0,10299.0,12173.0,-1.0] |0.0       |0        |\n|[1.0,1.0,10299.0,13487.0,-6.0] |0.0       |0        |\n|[1.0,1.0,10299.0,14747.0,-8.0] |0.0       |0        |\n|[1.0,1.0,10299.0,14747.0,-8.0] |0.0       |0        |\n|[1.0,1.0,10299.0,14747.0,-7.0] |0.0       |0        |\n+-------------------------------+----------+---------+\nonly showing top 10 rows"}], "metadata": {"scrolled": false, "cell_status": {"execute_time": {"duration": 2265.3359375, "end_time": 1577467697595.811}}, "collapsed": false}}, {"source": "### Compute Confusion Matrix Metrics\nClassifiers are typically evaluated by creating a *confusion matrix*, which indicates the number of:\n- True Positives\n- True Negatives\n- False Positives\n- False Negatives\n\nFrom these core measures, other evaluation metrics such as *precision* and *recall* can be calculated.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "tp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 1\").count())\nfp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 0\").count())\ntn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 0\").count())\nfn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 1\").count())\nmetrics = spark.createDataFrame([\n (\"TP\", tp),\n (\"FP\", fp),\n (\"TN\", tn),\n (\"FN\", fn),\n (\"Precision\", tp / (tp + fp)),\n (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\nmetrics.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+-------------------+\n|   metric|              value|\n+---------+-------------------+\n|       TP|            19386.0|\n|       FP|               76.0|\n|       TN|           648978.0|\n|       FN|           141568.0|\n|Precision| 0.9960949542698592|\n|   Recall|0.12044435055978726|\n+---------+-------------------+"}], "metadata": {"cell_status": {"execute_time": {"duration": 9313.825927734375, "end_time": 1577467727124.371}}, "collapsed": false}}, {"source": "### View the Raw Prediction and Probability\nThe prediction is based on a raw prediction score that describes a labelled point in a logistic function. This raw prediction is then converted to a predicted label of 0 or 1 based on a probability vector that indicates the confidence for each possible label value (in this case, 0 and 1). The value with the highest confidence is selected as the prediction.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "prediction.select(\"rawPrediction\", \"probability\", \"prediction\", \"trueLabel\").show(10, truncate=False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------------------------------------+-----------------------------------------+----------+---------+\n|rawPrediction                           |probability                              |prediction|trueLabel|\n+----------------------------------------+-----------------------------------------+----------+---------+\n|[1.593444678995949,-1.593444678995949]  |[0.8311001934688109,0.168899806531189]   |0.0       |0        |\n|[1.5118769235890168,-1.5118769235890168]|[0.8193392002639885,0.18066079973601137] |0.0       |0        |\n|[1.6241684307493662,-1.6241684307493662]|[0.8353692053583021,0.164630794641698]   |0.0       |0        |\n|[1.1638916514503854,-1.1638916514503854]|[0.7620391288695235,0.23796087113047645] |0.0       |1        |\n|[-10.217867341408695,10.217867341408695]|[3.651074880852677E-5,0.9999634892511915]|1.0       |1        |\n|[1.5583128948148666,-1.5583128948148666]|[0.8261111310635718,0.17388886893642813] |0.0       |0        |\n|[1.6323564641338346,-1.6323564641338346]|[0.8364921950677588,0.1635078049322412]  |0.0       |0        |\n|[1.6641382082294813,-1.6641382082294813]|[0.8407927264430656,0.15920727355693437] |0.0       |0        |\n|[1.6641382082294813,-1.6641382082294813]|[0.8407927264430656,0.15920727355693437] |0.0       |0        |\n|[1.6501039801248463,-1.6501039801248463]|[0.8389051031390478,0.16109489686095227] |0.0       |0        |\n+----------------------------------------+-----------------------------------------+----------+---------+\nonly showing top 10 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 1256.405029296875, "end_time": 1577467762744.785}}, "collapsed": false}}, {"source": "Note that the results include rows where the probability for 0 (the first value in the **probability** vector) is only slightly higher than the probability for 1 (the second value in the **probability** vector). The default *discrimination threshold* (the boundary that decides whether a probability is predicted as a 1 or a 0) is set to 0.5; so the prediction with the highest probability is always used, no matter how close to the threshold.", "cell_type": "markdown", "metadata": {}}, {"source": "### Review the Area Under ROC\nAnother way to assess the performance of a classification model is to measure the area under a ROC curve for the model. the spark.ml library includes a **BinaryClassificationEvaluator** class that you can use to compute this. The ROC curve shows the True Positive and False Positive rates plotted for varying thresholds.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 22, "cell_type": "code", "source": "evaluator = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naur = evaluator.evaluate(prediction)\nprint \"AUR = \", aur", "outputs": [{"output_type": "stream", "name": "stdout", "text": "AUR =  0.560163628533"}], "metadata": {"cell_status": {"execute_time": {"duration": 2281.031982421875, "end_time": 1577468540050.332}}, "collapsed": false}}, {"source": "### Change the Discrimination Threshold\nThe AUC score seems to indicate a reasonably good model, but the performance metrics seem to indicate that it predicts a high number of False Negative labels (i.e. it predicts 0 when the true label is 1), leading to a low Recall. You can affect the way a model performs by changing its parameters. For example, as noted previously, the default discrimination threshold is set to 0.5 - so if there are a lot of False Positives, you may want to consider raising this; or conversely, you may want to address a large number of False Negatives by lowering the threshold.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 23, "cell_type": "code", "source": "lr2 = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3, threshold=0.35)\npipeline2 = Pipeline(stages=[assembler, lr2])\nmodel2 = pipeline2.fit(train)\nnewPrediction = model2.transform(test)\nnewPrediction.select(\"rawPrediction\", \"probability\", \"prediction\", \"trueLabel\").show(10, truncate=False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------------------------------------+-----------------------------------------+----------+---------+\n|rawPrediction                           |probability                              |prediction|trueLabel|\n+----------------------------------------+-----------------------------------------+----------+---------+\n|[1.593444678995949,-1.593444678995949]  |[0.8311001934688109,0.168899806531189]   |0.0       |0        |\n|[1.5118769235890168,-1.5118769235890168]|[0.8193392002639885,0.18066079973601137] |0.0       |0        |\n|[1.6241684307493662,-1.6241684307493662]|[0.8353692053583021,0.164630794641698]   |0.0       |0        |\n|[1.1638916514503856,-1.1638916514503856]|[0.7620391288695235,0.23796087113047643] |0.0       |1        |\n|[-10.217867341408693,10.217867341408693]|[3.651074880852683E-5,0.9999634892511915]|1.0       |1        |\n|[1.5583128948148666,-1.5583128948148666]|[0.8261111310635718,0.17388886893642813] |0.0       |0        |\n|[1.6323564641338346,-1.6323564641338346]|[0.8364921950677588,0.1635078049322412]  |0.0       |0        |\n|[1.6641382082294813,-1.6641382082294813]|[0.8407927264430656,0.15920727355693437] |0.0       |0        |\n|[1.6641382082294813,-1.6641382082294813]|[0.8407927264430656,0.15920727355693437] |0.0       |0        |\n|[1.6501039801248463,-1.6501039801248463]|[0.8389051031390478,0.16109489686095227] |0.0       |0        |\n+----------------------------------------+-----------------------------------------+----------+---------+\nonly showing top 10 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 5306.22998046875, "end_time": 1577468549603.397}}, "collapsed": false}}, {"source": "Note that some of the **rawPrediction** and **probability** values that were previously predicted as 0 are now predicted as 1", "cell_type": "markdown", "metadata": {}}, {"execution_count": 24, "cell_type": "code", "source": "# Recalculate confusion matrix\ntp2 = float(newPrediction.filter(\"prediction == 1.0 AND truelabel == 1\").count())\nfp2 = float(newPrediction.filter(\"prediction == 1.0 AND truelabel == 0\").count())\ntn2 = float(newPrediction.filter(\"prediction == 0.0 AND truelabel == 0\").count())\nfn2 = float(newPrediction.filter(\"prediction == 0.0 AND truelabel == 1\").count())\nmetrics2 = spark.createDataFrame([\n (\"TP\", tp2),\n (\"FP\", fp2),\n (\"TN\", tn2),\n (\"FN\", fn2),\n (\"Precision\", tp2 / (tp2 + fp2)),\n (\"Recall\", tp2 / (tp2 + fn2))],[\"metric\", \"value\"])\nmetrics2.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+------------------+\n|   metric|             value|\n+---------+------------------+\n|       TP|           42044.0|\n|       FP|             127.0|\n|       TN|          648927.0|\n|       FN|          118910.0|\n|Precision| 0.996988451779659|\n|   Recall|0.2612174907116319|\n+---------+------------------+"}], "metadata": {"cell_status": {"execute_time": {"duration": 5279.468994140625, "end_time": 1577468556795.504}}, "collapsed": false}}, {"source": "Note that there are now more True Positives and less False Negatives, and Recall has improved. By changing the discrimination threshold, the model now gets more predictions correct - though it's worth noting that the number of False Positives has also increased.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": "aur2 = evaluator.evaluate(newPrediction)\nprint \"AUR = \", aur2", "outputs": [{"output_type": "stream", "name": "stdout", "text": "AUR =  0.630510910661"}], "metadata": {"cell_status": {"execute_time": {"duration": 2265.494873046875, "end_time": 1577468564745.27}}, "collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}